# AI BLOG NOTES

## The Sequence: Moving Past RLHF: In 2025 We Will Transition from Preference Tuning to Reward Optimization in Foundation Models
**https://thesequence.substack.com/p/moving-past-rlhf-in-2025-we-will?utm_campaign=email-half-post&r=4lniy8&utm_source=substack&utm_medium=email**

1. There is a transition from an emphasis in pre-training to post-training in Foundational Models.

2. The release of models like GPT-o1 and the initial details about GPT-o3 as well as frameworks such as TÃ¼lu 3 really provide a glimpse of that trajectory. 

3. Even with the post-training space we are seeing super intriguing changes. 

4. One of thsi is the transition from preference-tuning with methods such as the famous RLHF to reward modeling.

5. This blog covers the following: 
    - how preference tuning paved way for the reward optimisation 
    - the impact and limitations of RLHF
    - the emergence of new reward models that aim to capture complex human values more effectively

6. Foundational models learn statistcal patterns by being trained on massive corpus of text data, and they can be used on a variety of tasks by minimal fine-tuning. However, as they grow more powerful, the need to align their outputs with human goals, values, and preferences becomes both more urgent and more challenging.

7. Preference tuning was the de facto approach to alignment relying on human-annotated datasets to guide model behaviour. Although it yield significant benefits in terms of helpfulness and safety, it struggles to incorporate the full scope of human intention, values and context specific nuances.
